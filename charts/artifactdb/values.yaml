global:
  # -- Instance ID, lowercase, no space, 16 chars max.
  instance_id: "artifactdb"
  # -- Instance version, used as a path prefix in the URL.
  instance_version: "v1"
  # -- Specify the environment. Value must match a config-{env}.yml file used as a ConfigMap content.
  env: null
  # -- Optional: specify the environment name to select sealed-secret file. If not set, `global.env` is used.
  envSecrets: null
  # -- Runs in standalone mode, using minimal/vanilla base docker image (no customization, set to false if using a
  # custom image).
  standalone:
    enabled: false
    src_path: "/app/src"
  # -- Used for dev. environment with local Kubernetes cluster like k3d, name of the root folder containing code for that
  # instance.  When `mountLocalCode` is used, that folder will be mounted into pods.
  # environment.
  src_folder: null
  # initial PYTHONPATH, to adjust to custom installation
  pythonpath: "/app:/app/lib"


# -- Force namespace name, otherwise taken from Helm CLI
namespaceOverride:

# -- ArtifactDB image (without tag) to use.
image: ghcr.io/artifactdb/artifactdb-docker/artifactdb
imagePullPolicy: IfNotPresent
# -- ArtifactDB image tag to use.
tag: main-be0d4e0

# -- In a local development environment like k3d, ability to mount local source code directly into containers
mountLocalCode: false
# -- In a local development environment like k3d, ability to mount library code
# (artifactdb-{backend,identifiers,utils},...) into containers. Usefull to share the same libs accross different
# deployments.
mountLocalLib: false

frontend:
  # -- Install a Deployment for frontend pods. Disable to override the main chart with a custom deployment.
  enabled: true
  # -- Number of replicas for frontend pods serving REST API.
  replicaCount: 2
  service:
    # -- Install an service (disable to override the servive definition in the main chart)
    enabled: true
    #type: NodePort
    #nodePort: 30800
    type: ClusterIP
    port: 8000
    targetPort: 8000
  # -- Activate readiness probe, checking availability of /status endpoint. Recommended for production.
  readinessProbe:
    enabled: true
  autoreload:
    enabled: false
  # -- Number of gunicorn workers serving the API. `null` for unlimited where gunicorn itself sets it according to
  # available cores.
  workers: null

backend:
  # -- Install a Deployment for backeend pods (celery workers + beat). Disable to override the main chart with a custom deployment.
  enabled: true
  # -- Number of replicas for backend pods performing async tasks, such as indexing.
  replicaCount: 2
  service:
    port: 5555
  # -- Activate readiness probe, checking availability of RabbitMQ.
  readinessProbe:
    enabled: false

flower:
  # Optional image to use for Flower deployment
  image: mher/flower:0.9.5
  # -- Number of replicas for Flower (UI on top of RabbitMQ). 1 is enough...
  replicaCount: 1
  service:
    type: ClusterIP
    targetPort: 5555
    port: 8888

unittest:
  # -- Deploy a `testsuite` pod used to run unit tests.
  enabled: false
  # -- pytest command line
  args: "pytest tests"
  # -- Automatically runs the test suite as a Kubernetes Job. If false, spin up a Pod that can be used interactively.
  autorun: false
  # -- Remove test suite code, which can be destructive and be dangerous to keep in a non-dev environment.
  remove: true
  base_url: null

admin:
  # -- Deploy an "admin" pod allowing to expose access to a Pod's terminal through a web browser.
  enabled: true
  # -- Unless specificaly requested, no admin pod deployed for security reasons, to avoid unnecessary exposure.
  replicaCount: 0
  service:
    type: ClusterIP
    targetPort: 3000
    port: 3000
  wetty:
    # Optional image to use for wetty deployment
    image: wettyoss/wetty

# -- Force chart name, otherwise generated by Helm
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # -- Specifies whether a service account should be created
  create: false
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations:
  # Enable auto-reloader of deployments when configmaps change
  reloader.stakater.com/auto: "true"

#podSecurityContext: {}
#  # fsGroup: 2000
#
#securityContext: {}
#  # capabilities:
#  #   drop:
#  #   - ALL
#  # readOnlyRootFilesystem: true
#  # runAsNonRoot: true
#  # runAsUser: 1000


ingress:
  enabled: false
  # -- Specify the hostname for ingress route rules. Working in combination with `prefix`, see below.
  hostname: null
  # -- Specify the path prefix for ingress route rules. If `instance_version` is specified, it is appended to the path
  # prefix.  The combination of the 3 parameters define the ingress rules.
  # Example 1: api name as a subdomain
  # - ingress.hostname: myapi.mycompany.com
  # - instance_version: v3
  # - ingress.prefix: null
  # => route: myapi.mycompany.com/v3
  # Example 2: api name as a path prefix
  # - ingress.hostname: mycompany.com
  # - instance_version: v7
  # - ingress.prefix: myapi
  # => route: mycompany.com/myapi/v3
  prefix: null


network_policy:
  enabled: true
  # -- If using an ingress controller, traffic needs to be allowed between the namespace where the controller is
  # deployed, and the namespace where ArtifactDB instance is deployed. Ignored and not set by default.
  ingress_namespace: null

# -- Specifies if the deployment is running within a CI pipeline. This adds a label `ci: true` in the deployments, that
# can indirectly be used to avoid exposing the REST API unintentionally (eg. with a combination of tolerations and k8s
# nodes labelling, highly depends on the k8s cluster setup though, ymmv...)
ci: false


# -- Specifies pod resources.
resources: {}
#  limits:
#    cpu: null
#    memory: null
#  requests:
#    cpu: null
#    memory: null

# Not used for now.
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

#tolerations: []

affinity:
  # -- Default pod anti-affinity rule, defaulting to deploying in multiple zones if available.
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          topologyKey: topology.kubernetes.io/zone
        weight: 100


# Subcharts configuration
redis:
  usePassword: false
  fullnameOverride: redis
  cluster:
    # -- Enable Redis cluster deployment (recommended for production).
    enabled: true
  master:
    nodeSelector: {}
  slave:
    nodeSelector: {}

rabbitmq:
  fullnameOverride: rabbitmq
  # -- Number of masters (2 recommended for production)
  replicaCount: 2
  auth:
    # whatever...
    username: user
    password: abc123
    erlangCookie: RABBITMQ_ERLANG_COOKIE
  persistence:
    enabled: true
  clustering:
    forceBoot: yes
  image:
    # -- Optional docker registry secret to pull the rabbitmq image, in case of "anon pull limits reached" errors...
    pullSecrets: []
  nodeSelector: {}

# -- For local development/test purposes, deploys a single node Elasticsearch "cluster".
elasticsearch:
  enabled: false
  imageTag: "7.4.0"
  replicas: 1
  minimumMasterNodes: 1
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

postgresql:
  # -- Deploys a local PostgreSQL database. Mostly for dev purposes, production deployments should preferrably use an
  # externally maintained instance (eg. AWS RDS), referenced in the instance's configuration file.
  enabled: false
  usePasswordFile: true
  existingSecret: pg-credentials
  fullnameOverride: postgresql

adb-maintainer-operator:
  # -- Experimental: deploys an Olympus Maintainer Operator, which takes care of model generation from schema updates,
  # mapping upgrades, and reindexing.
  enabled: false

